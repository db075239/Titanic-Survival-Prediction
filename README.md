# ðŸš¢ Titanic Survival Prediction

Welcome to my take on the well-known **Kaggle Titanic Machine Learning Challenge**. This project explores a variety of machine learning models and techniques to predict passenger survival aboard the Titanic â€” one of the most well-known tragedies in modern history.

---

## ðŸ§­ Table of Contents
- [ðŸ“Œ Introduction](#-introduction)
- [ðŸŽ¯ Project Goal](#-project-goal)
- [ðŸ§¾ Dataset](#-dataset)
- [âš™ï¸ Methodology](#-methodology)
- [ðŸ¤– Models Implemented](#-models-implemented)
- [ðŸ“Š Results](#-results)
- [ðŸ“ Files in this Repository](#-files-in-this-repository)
- [ðŸ“ Conclusion](#-conclusion)

---

## ðŸ“Œ Introduction

The **Titanic dataset** is a classic benchmark in the field of data science and machine learning. Its real-world significance and relatively well-structured data make it perfect for introducing and demonstrating core ML concepts.

This project applies a full machine learning pipeline â€” from preprocessing to model evaluation â€” to solve a **binary classification problem**: predicting survival on the Titanic.

---

## ðŸŽ¯ Project Goal

The objective is to **predict whether a given passenger survived** the Titanic disaster based on personal and travel-related attributes.  
This is a supervised classification task with two possible outcomes:
- **1**: Survived
- **0**: Did not survive

---

## ðŸ§¾ Dataset

The dataset is sourced from [Kaggleâ€™s Titanic Challenge](https://www.kaggle.com/competitions/titanic). It includes demographic and socio-economic details of Titanic passengers.

Key features include:
- `Age`
- `Sex`
- `Fare`
- `Pclass` (Ticket Class)
- `SibSp`, `Parch` (Family relationships)
- `Embarked`, `Cabin`, `Ticket` and more.

### ðŸ—‚ Files
- `train.csv` â€” For training and validating models.
- `test.csv` â€” For evaluating model predictions on unseen data.

---

## âš™ï¸ Methodology

A typical machine learning workflow was followed:

1. **ðŸ§¹ Data Preprocessing**
   - Imputation of missing values
   - Label encoding for categorical variables
   - Feature engineering (e.g., `FamilySize`, `IsAlone`)
2. **âš–ï¸ Feature Scaling**
   - Standardization to ensure balanced input for distance-based models like KNN.
3. **ðŸ§  Model Implementation**
   - Training multiple ML algorithms to compare their performance.
4. **ðŸ“ˆ Model Evaluation**
   - Using metrics like accuracy, precision, recall, and confusion matrix to measure effectiveness.

---

## ðŸ¤– Models Implemented

Three machine learning algorithms were trained and compared:

- **ðŸ”¹ Logistic Regression**
  - A simple yet powerful linear model for binary classification.
  - Pros: Interpretable and fast.
  
- **ðŸŒ² Random Forest**
  - An ensemble of decision trees; handles overfitting better.
  - Pros: High accuracy and robust performance.

- **ðŸ‘¥ K-Nearest Neighbors (KNN)**
  - A lazy learning algorithm that classifies based on proximity in feature space.
  - Pros: Performs well with properly scaled data.

---

## ðŸ“Š Results

The models delivered promising results, with **Random Forest** outperforming others overall.

| Model               | Accuracy | Notes                          |
|--------------------|----------|--------------------------------|
| **Random Forest**   | âœ… High   | Best balance of accuracy and robustness |
| **KNN**             | âœ… Good   | Improved with scaling          |
| **Logistic Regression** | âœ”ï¸ Moderate | Simple and interpretable       |

> ðŸ” Feature engineering (e.g., combining `SibSp` and `Parch`) and scaling significantly improved performance.

---

## ðŸ“ Files in this Repository

| File | Description |
|------|-------------|
| `Projekt.ipynb` | The main Jupyter Notebook with the full pipeline: data loading, preprocessing, training, and evaluation. |
| `AIS_Report.pdf` | A detailed written report covering methodology, background theory, and results. |
| `train.csv` | Training data for the Titanic challenge. |
| `test.csv` | Test data for final predictions. |

---

## ðŸ“ Conclusion

This project successfully demonstrates how classic machine learning techniques can be applied to real-world problems. It reinforces the importance of:
- Data preprocessing
- Feature scaling and engineering
- Model comparison and evaluation

### ðŸš€ Future Work
To further improve performance, future enhancements could include:
- Feature extraction from `Cabin` and `Ticket` fields
- Ensemble stacking or boosting (e.g., XGBoost, LightGBM)
- Incorporating deep learning models for comparison

---

> *"A smooth sea never made a skilled sailor."*  
> â€• Franklin D. Roosevelt

---
